{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi di documenti simili #\n",
    "\n",
    "Una delle applicazioni delle tecniche di analisi dei documenti simili &egrave; legata alle procedure volte a eliminare record che si riferiscono alla stessa entit&agrave; quando si fondono due o pi&ugrave; fonti di dati. A tali procedure ci si riferisce utilizzando tipicamente il nome *entity resolution* o *record linkage*, ma nel gergo tecnico esiste una vasta gamma di termini pi&ugrave; o meno equivalenti, quali per esempio *entity disambiguation*, *entity linking*, *duplicate detection*, *deduplication*, *record matching*, *conflation*, *reconciliation* o, pi&ugrave; genericamente *data integration*)\n",
    "\n",
    "Si parla dunque di entity resolution quando si vogliono rilevare record in una fonte dati che si riferiscono tutti a una stessa entit&agrave;, tipicamente a seguito dell'integrazione di fonti dati diverse e a fronte di descrizioni non identiche di uno stesso oggetto.\n",
    "\n",
    "## Strumenti\n",
    "Questo tutorial &egrave; basato sull'uso di funzionalitÃ  base di Python 2.7, di Spark 1.4.1 associato alle API pySpark e della libreria grafica matplotlib. Verr&agrave; fatto uso delle librerie standard per le espressioni regolari: nel caso in cui sia necessario un ripasso, il sito [regex101](https://regex101.com/) permette di applicare espressioni regolari su stringhe in modo interattivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File\n",
    "I dati utilizzati fanno parte del progetto [metric-learning](https://code.google.com/archive/p/metric-learning/source). I file corrispondenti, memorizzati nella directory `data`, sono i seguenti:\n",
    "\n",
    "* `Google.csv`, dataset di prodotti estratto da Google,\n",
    "* `Amazon.csv`, dataset di prodotti estratto da Amazon,\n",
    "* `Google_small.csv`, \\\\(200\\\\) record campionati da `Google.csv`,\n",
    "* `Amazon_small.csv` \\\\(200\\\\) record campionati da `Amazon.csv`,\n",
    "* `Amazon_Google_perfectMapping.csv`, mapping tra i record dei due dataset che si riferiscono a uno stesso prodotto,\n",
    "* `stopwords.txt`, un elenco di stop word per la lingua inglese.\n",
    "\n",
    "Useremo i file contenenti i record campionati nella prima parte del tutorial, cosÃ¬ da poter sperimentare senza che l'elaborazione richieda di attendere troppo tempo. Il file che indica quali coppie di record si riferiscano allo stesso prodotto (informazione che prende il nome di *gold standard* o *ground truth*) verrÃ  usato per valutare la performance dell'algoritmo proposto.\n",
    "\n",
    "Memorizziamo i nomi di questi file in altrettante costanti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "\n",
    "base_dir = os.path.join(DATA_DIR)\n",
    "\n",
    "GOOGLE_PATH = 'Google.csv'\n",
    "GOOGLE_SMALL_PATH = 'Google_small.csv'\n",
    "AMAZON_PATH = 'Amazon.csv'\n",
    "AMAZON_SMALL_PATH = 'Amazon_small.csv'\n",
    "GOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\n",
    "STOPWORDS_PATH = 'stopwords.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Operazioni preliminari ##\n",
    "\n",
    "Iniziamo creando un RDD per ciascuno dei due dataset. In essi, ogni linea fa riferimento a un prodotto, utilizzando il formato\n",
    "\n",
    "    `\"id\",\"nome\",\"descrizione\",\"produttore\",\"prezzo\"`\n",
    "\n",
    "descritto nella prima linea di ogni file (che non dovr&agrave; quindi essere considerata). Va anche notato che i doppi apici fanno parte delle varie stringhe e che gli identificativi hanno una forma differente nei due dataset: sono URL per i dati di Google e codici alfanumerici per i dati di Amazon.\n",
    "\n",
    "Iniziamo costruendo degli RDD in cui gli elementi siano delle coppie in cui la chiave indica l'identificativo e il valore il resto della linea. Il parsing dei doppi apici va considerato con attenzione, perch&eacute; quando un dato &egrave; mancante (come capita per esempio con il produttore) i file contengono una stringa vuota non delimitata da doppi apici. Vale quindi la pena utilizzare le virgole come punti di riferimento per costruire un'espressione regolare che indichi il formato di una linea dei file (cinque campi separati da virgola, dei quali i primi due non possono essere mancanti):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "DATAFILE_PATTERN = '^(.+),(.+),(.*),(.*),(.*)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta verificato che una linea corrisponde a questa espressione regolare potremo procedere, ove serve, a eliminare i doppi apici, utilizzando la funzione che segue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotes(s):\n",
    "    \"\"\" Remove quotation marks from an input string\n",
    "    \n",
    "    Args:\n",
    "        s (str): input string that might have the quote \"\" characters\n",
    "    \n",
    "    Returns:\n",
    "        str: a string without the quote characters\n",
    "    \"\"\"\n",
    "    return ''.join(i for i in s if i!='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamo ora pronti a scrivere la funzione che si occuperÃ  del parsing di una linea, verificando innanzitutto che quest'ultima segua il formato richiesto (restituendo una coppia contenente la linea e il valore speciale `-1` altrimenti) e identificando la prima riga dei file che contiene le intestazioni (restituendo una coppia contenente la linea e il valore speciale `0`). Infine i valori per i campi andranno recuperati al fine di costruire una coppia avente come chiave l'identificativo del prodotto (dopo avere eliminato i doppi apici) e come valore i restanti campi giustapposti. Tale oggetto dovr&agrave; essere restituito inserendolo come primo elemento in una coppia, il cui secondo elemento sar&agrave; il valore speciale `1`. In questo modo sar&agrave; possibile verificare quante linee dei file sono state convertite correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datafile_line(datafile_line):\n",
    "    \"\"\" Parse a line of the data file using the specified regular expression\n",
    "    pattern\n",
    "    \n",
    "    Args:\n",
    "        datafileLine (str): input string that is a line from the data file\n",
    "    \n",
    "    Returns:\n",
    "        str: a string parsed using the given regular expression and without\n",
    "             the quote characters\n",
    "    \"\"\"\n",
    "    \n",
    "    match = re.search(DATAFILE_PATTERN, datafile_line.decode('utf-8'))\n",
    "    if match is None:\n",
    "        print('Invalid datafile line: {}'.format(datafile_line))\n",
    "        return (datafile_line, -1)\n",
    "    elif match.group(1) == '\"id\"':\n",
    "        print('Header datafile line: {}'.format(datafile_line))\n",
    "        return (datafile_line, 0)\n",
    "    else:\n",
    "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n",
    "        return ((remove_quotes(match.group(1)), product), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo &egrave; quello di definire una funzione che crei un RDD contenente un elemento per ogni linea di un file, per poi mapparvi sopra la funzione `parse_dataline_file` appena definita, mettendo in *cache* il risultato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    \"\"\" Parse a data file\n",
    "    \n",
    "    Args:\n",
    "        filename (str): input file name of the data file\n",
    "        \n",
    "    Returns:\n",
    "        RDD: a RDD of parsed lines\n",
    "    \"\"\"\n",
    "    \n",
    "    return (sc\n",
    "            .textFile(filename, 4, 0)\n",
    "            .map(parse_datafile_line)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&Egrave; ora possibile costruire una funzione che dato il nome di un dataset si occupa di creare il corrispondente RDD, e verifica che non vi siano stati errori di conversione. Ci&ograve; viene fatto:\n",
    "* costruendo l'RDD che si ottiene filtrando il numero di coppie che contengono il valore speciale `-1` e selezionando il loro primo elemento (ottenendo le linee che non sono convertibili);\n",
    "* stampando il contenuto di (al pi&ugrave;) dieci linee siffatte;\n",
    "* costruendo l'RDD che corrisponde alle coppie che contengono il valore `0` e selezionandone il primo elemento (ovvero una coppia `(id, prodotto)` ottenuta a seguito di una conversione valida)\n",
    "* inserendo l'RDD dei valori convertiti in *cache*;\n",
    "* verificando che non siano avvenuti errori di conversione e che tutte le righe del file siano state effettivamente convertite;\n",
    "* restituendo l'RDD dei valori convertiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\" Load a data file\n",
    "    \n",
    "    Args:\n",
    "        path (str): input file name of the data file\n",
    "        \n",
    "    Returns:\n",
    "        RDD: a RDD of parsed valid lines\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = os.path.join(base_dir, path)\n",
    "    raw = parse_data(filename).cache()\n",
    "    \n",
    "    failed = (raw\n",
    "              .filter(lambda s: s[1] == -1)\n",
    "              .map(lambda s: s[0]))\n",
    "    \n",
    "    for line in failed.take(10):\n",
    "        #print('{} - Invalid datafile line: {}'.format(path, line))\n",
    "        print(line)\n",
    "        \n",
    "    valid = (raw\n",
    "             .filter(lambda s: s[1] == 1)\n",
    "             .map(lambda s: s[0])\n",
    "             .cache())\n",
    "    print('{} - Read {} lines, successfully parsed {} lines, '\n",
    "           'failed to parse {} lines'.format(path,\n",
    "                                             raw.count(),\n",
    "                                             valid.count(),\n",
    "                                             failed.count()))\n",
    "    assert failed.count() == 0\n",
    "    assert raw.count() == (valid.count() + 1)\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizzando le funzioni scritte si possono effettuare le conversioni dei dataset che verranno utilizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_small = load_data(GOOGLE_SMALL_PATH)\n",
    "google = load_data(GOOGLE_PATH)\n",
    "amazon_small = load_data(AMAZON_SMALL_PATH)\n",
    "amazon = load_data(AMAZON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifichiamo il contenuto di alcuni record nei due dataset convertiti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in google_small.take(3):\n",
    "    print('google: {}: {}\\n'.format(line[0], line[1]))\n",
    "\n",
    "for line in amazon_small.take(3):\n",
    "    print('amazon: {}: {}\\n'.format(line[0], line[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Codificare documenti tramite *bag of words* ##\n",
    "\n",
    "Un modo naturale di effettuare entity resolution su documenti testuali &egrave; quello di considerare ogni record come una stringa e utilizzare una distanza tra stringhe per calcolare la similiarit&agrave; tra due documenti. In particolare utilizzeremo il cosiddetto approccio *bag of words*, in cui un documento testuale &egrave; codificato in termini di un insieme di parole o, meglio ancora, *token* che occorrono in esso.\n",
    "\n",
    "> **Nota**: si preferisce parlare di *token* per enfatizzare che non si tratta necessariamente di una parola, bens&igrave; di un Â«atomoÂ» indivisibile che potrebbe essere una parola ma anche una sottostringa di lunghezza fissata (\\\\(k\\\\)-gramma) che occorre nel testo.\n",
    "\n",
    "I token divengono gli elementi che ci permettono di effettuare un confronto tra documenti, la cui similarit&agrave; puÃ² essere espressa in termini del numero di token in comune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Estrarre token da una stringa ###\n",
    "\n",
    "Implementiamo una funzione `simple_tokenize` che data una stringa restituisce la lista dei token in essa contenuti, intesi come parole delimitate da spzi bianchi. Questa funzione utilizza opportunamente le espressioni regolari e sar&agrave; *case insensitive*, prendendosi inoltre cura di non considerare eventuali token vuoti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "def simple_tokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    return [s for s in re.split(split_regex, string.lower()) if s != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifichiamo che la funzione si comporti correttamente su un semplice caso di test, che non consideri gli spazi vuoti e che non elimini eventuali token duplicati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_tokenize(quickbrownfox))\n",
    "\n",
    "assert(simple_tokenize(quickbrownfox) ==\n",
    "       ['a','quick','brown','fox','jumps','over','the','lazy','dog'])\n",
    "assert(simple_tokenize(' ') == [])\n",
    "assert(simple_tokenize('!!!!123A/456_B/789C.123A') == ['123a','456_b','789c','123a'])\n",
    "assert(simple_tokenize('fox fox') == ['fox', 'fox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Eliminare le stop word ###\n",
    "\n",
    "Si indicano con il termine *stop word* le parole che appaiono comunemente nei testi scritti in una fissata lingua, e che dunque non apportano particolare contenuto informativo. Per tale motivo &egrave; opportuno eliminare tali parole prima di estrarre token da un documento. Utilizzando il file `stopwords.txt` implementiamo la funzione `tokenize` che si comporta in modo analogo a `simple_tokenize` ma che ignora le *stop word*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopfile = os.path.join(base_dir, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "print('These are the stopwords: {}'.format(stopwords))\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    \n",
    "    Args:\n",
    "        string (str): input string\n",
    "        \n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    return [s for s in re.split(split_regex, string.lower())\n",
    "            if s != '' and not s in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso, verifichiamo alcuni semplici test: la funzione deve eliminare le *stop word* ma considerare tutti gli altri termini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(quickbrownfox))\n",
    "\n",
    "assert(tokenize(\"Why a the?\") == [])\n",
    "assert(tokenize(\"Being at the_?\") == ['the_'])\n",
    "assert(tokenize(quickbrownfox) == ['quick','brown','fox','jumps','lazy','dog'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Estrarre i token dai dataset campionati ###\n",
    "\n",
    "Iniziamo a estrarre i token dai due dataset campionati, applicando `tokenize` a ogni elemento dell'RDD corrispondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_rec_to_token = amazon_small.map(lambda s: (s[0], tokenize(s[1])))\n",
    "google_rec_to_token = google_small.map(lambda s: (s[0], tokenize(s[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come prima analisi, provvediamo a contare il numero totale di token contenuti nei due dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(vendorRDD):\n",
    "    \"\"\" Count and return the number of tokens\n",
    "    Args:\n",
    "        vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output\n",
    "    Returns:\n",
    "        count: count of all tokens\n",
    "    \"\"\"\n",
    "    return vendorRDD.map(lambda s: len(s[1])).reduce(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = count_tokens(amazon_rec_to_token) + count_tokens(google_rec_to_token)\n",
    "print('There are {} tokens in the combined datasets'.format(total_tokens))\n",
    "assert(total_tokens == 22520)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d Determinare quale documento ha pi&ugrave; token ###\n",
    "\n",
    "Quale documento del dataset di Amazon ha il maggior numero di token? Per saperlo basta ordinare opportunamente i record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_biggest_record(vendorRDD):\n",
    "    \"\"\" Find and return the record with the largest number of tokens\n",
    "    \n",
    "    Args:\n",
    "        vendorRDD (RDD of (recordId, tokens)): input Pair Tuple of record ID and tokens\n",
    "        \n",
    "    Returns:\n",
    "        list: a list of 1 Pair Tuple of record ID and tokens\n",
    "    \"\"\"\n",
    "    return vendorRDD.takeOrdered(1, key = lambda s: -1*len(s[1]))\n",
    "\n",
    "biggest_record_amazon = find_biggest_record(amazon_rec_to_token)\n",
    "print('The Amazon record with ID \"{}\"'\n",
    "      ' has the most tokens ({})'.format(biggest_record_amazon[0][0],\n",
    "                                         len(biggest_record_amazon[0][1])))\n",
    "\n",
    "assert(biggest_record_amazon[0][0] == 'b000o24l3q')\n",
    "assert(len(biggest_record_amazon[0][1]) == 1547)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Uso della misura TF.IDF ##\n",
    "\n",
    "Per migliorare la tecnica basata su *bag of words* &egrave; importante dare pesi diversi ai token contenuti in un documento, in modo da riflettere quali siano i termini pi&ugrave; rilevanti. Un'euristica che permette di capire quali siano questi termini &egrave; la *misura TF.IDF* (Term Frequency times Inverse Document Frequency), ottenuta moltiplicando tra loro due indicatori definiti qui di seguito.\n",
    "\n",
    "### Term Frequency ###\n",
    "\n",
    "La *Term Frequency* d&agrave; maggior peso a token che tendono a occorrere pi&ugrave; volte in uno stesso documento. Viene calcolata come la frequenza relativa del token nel documento. In altre parole, se il documento \\\\(d\\\\) contiene \\\\(100\\\\) token e tra questi il token \\\\(t\\\\) occorre in \\\\(d\\\\) cinque volte, la Term Frequency di \\\\(t\\\\) in \\\\(d\\\\) vale \\\\(\\mathrm{TF}(t, d) = \\frac{5}{100} = \\frac{1}{20}\\\\).\n",
    "\n",
    "### Inverse Document Frequency ###\n",
    "\n",
    "La *Inverse Document Frequency* d&agrave; un peso alto a token che occorrono raramente su un dataset. Dato un token \\\\(t\\\\) e un insieme di documenti \\\\(U\\\\), la Inverse Document Frequency di \\\\(t\\\\) &egrave; uguale a \\\\(\\mathrm{IDF}(t) = \\frac{N}{n(t)}\\\\), dove \\\\(N\\\\) &egrave; il numero totale di documenti in \\\\(U\\\\) e \\\\(n(t)\\\\) indica il numero di documenti in \\\\(U\\\\) che contengono \\\\(t\\\\). \n",
    "\n",
    "### TF.IDF ###\n",
    "\n",
    "La misura TF.IDF di un token \\\\(t\\\\) in un documento \\\\(d\\\\) &egrave; pari al prodotto tra la Term Frequency e la Inverse Document Frequency: \\\\(\\mathrm{TF.IDF}(t, d)= \\mathrm{TF}(t, d) \\cdot \\mathrm{IDF(t)}\\\\). Informalmente, un'elevata TF.IDF indica un token che occorre spesso in un documento ma raramente negli altri, e quindi andr&agrave; dato piÃ¹ peso, nel calcolo della similarit&agrave; tra due documenti, al fatto che entrambi contengano un termine avente un'alto valore della misura TF.IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementare una funzione per la Term Frequency ###\n",
    "\n",
    "Implementiamo una funzione `tf` che data una lista di token restituisce un dizionario che trasforma ogni token nella corrispondente misura TF. La funzione opera nel modo seguente:\n",
    "\n",
    "* crea un dizionario vuoto;\n",
    "* per ogni token nella lista passata come argomento, lo aggiunge come chiave associata a \\\\(1\\\\) se gi&agrave; non occorre nel dizionario, altrimenti incrementa di un'unitÃ  il valore associato;\n",
    "* per ogni token nel dizionario, divide il valore associato per il numero totale di token nella lista passata come argomento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(tokens):\n",
    "    \"\"\" Compute TF\n",
    "    \n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "        \n",
    "    Returns:\n",
    "        dictionary: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    val = {}\n",
    "    for t in tokens:\n",
    "        if t in val:\n",
    "            val[t] += 1\n",
    "        else:\n",
    "            val[t] = 1.0\n",
    "    for t in val:\n",
    "        val[t] /= len(tokens)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso, verifichiamo che la funzione `tf` passi alcuni test di base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf(tokenize(quickbrownfox)))\n",
    "tf_test = tf(tokenize(quickbrownfox))\n",
    "assert(tf_test == {'brown': 0.16666666666666666, 'lazy': 0.16666666666666666,\n",
    "                             'jumps': 0.16666666666666666, 'fox': 0.16666666666666666,\n",
    "                             'dog': 0.16666666666666666, 'quick': 0.16666666666666666})\n",
    "tf_test2 = tf(tokenize('one_ one_ two!'))\n",
    "assert(tf_test2 == {'one_': 0.6666666666666666, 'two': 0.3333333333333333})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creare un corpus di documenti ###\n",
    "\n",
    "Il passo successivo Ã¨ quello di creare un RDD che funga da *corpus* contenente tutti i documenti che vogliamo analizzare, e quindi sia il risultato della fusione tra il dataset di Amazon e quello di Google. Ogni elemento di questo nuovo RDD sar&agrave; una coppia contenente come chiave l'identificatore di un prodotto e come valore la sua descrizione (a seguito delle operazioni di ripulitura)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusRDD = google_rec_to_token.union(amazon_rec_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(corpusRDD.count() == 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implementare una funzione per l'Inverse Document Frequency ###\n",
    "\n",
    "Implementiamo una funzione `idfs` che assegna il valore di Inverse Document Frequency a ogni token che occorre in un corpus di documenti, restituendo un RDD di coppie \\\\((t, i)\\\\) dove \\\\(t\\\\) e \\\\(i\\\\) sono rispettivamente un token e il relativo valore di IDF.\n",
    "\n",
    "La funzione eseguir&agrave; i seguenti passi:\n",
    "\n",
    "* calcolare \\\\(N\\\\), il numero di documenti del corpus;\n",
    "* creare un RDD che per ogni documento del corpus contenga tutti i token unici che vi occorrono; in altre parole, per ogni documento sarÃ  necessario includere ogni token una volta sola, *anche nel caso in cui esso occorra piÃ¹ volte nel documento*;\n",
    "* per ognuno dei token unici \\\\(t\\\\), contare quante volte occorre nel corpus per calcolare \\\\(n(t)\\\\) e successivamente l'Inverse Document Frequency per \\\\(t\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idfs(corpus):\n",
    "    \"\"\" Compute IDF\n",
    "\n",
    "    Args:\n",
    "        corpus (RDD): input corpus\n",
    "\n",
    "    Returns:\n",
    "        RDD: a RDD of (record ID, IDF value)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = corpus.count() * 1.0\n",
    "    unique_tokens = corpus.map(lambda s: list(set(s[1])))\n",
    "    token_count_pair_tuple = unique_tokens.flatMap(lambda t: [(s, 1) for s in t])\n",
    "    token_sum_pair_tuple = token_count_pair_tuple.reduceByKey(lambda a, b: a+b)\n",
    "    return token_sum_pair_tuple.map(lambda s: (s[0], N/s[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la funzione `idfs` &egrave; possibile calcolare i valori IDF per tutti i token in `corpusRDD`, l'unione dei due dataset campionati, e determinare il numero di token unici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs_small = idfs(corpusRDD)\n",
    "unique_token_count = idfs_small.count()\n",
    "\n",
    "print('There are {} unique tokens'\n",
    "      ' in the small datasets.'.format(unique_token_count))\n",
    "\n",
    "assert(unique_token_count == 4772)\n",
    "token_smallest_IDF = idfs_small.takeOrdered(1, lambda s: s[1])[0]\n",
    "assert(token_smallest_IDF[0] == 'software')\n",
    "assert(abs(token_smallest_IDF[1] - 4.25531914894) < 0.0000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Determinare i token con i pi&ugrave; piccoli valori IDF ###\n",
    "\n",
    "Ordinando opportunamente l'RDD che contiene i valori IDF &egrave; possibile stampare gli \\\\(11\\\\) token con i valori IDF pi&ugrave; bassi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_IDF_tokens = idfs_small.takeOrdered(11, lambda s: s[1])\n",
    "print(small_IDF_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Istogramma dei valori IDF ###\n",
    "\n",
    "Possiamo ora visualizzare un istogramma dei valori IDF nel corpus, utilizzando la libreria `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "small_idf_values = idfs_small.map(lambda s: s[1]).collect()\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.hist(small_idf_values, 50, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Implementare una funzione per il calcolo di TF.IDF ###\n",
    "\n",
    "Utilizziamo la fuzione `tf` per implementare una nuova funzione `tfidf` che, dati in ingresso una lista di token in un documento e il dizionario restituito da `idfs`, restituisce un dizionario che associa ai token nella lista il corrispondente valore della misura TF.IDF.\n",
    "\n",
    "La funzione si occuper&agrave; di\n",
    "* calcolare il TF per tutti i token nella lista, e\n",
    "* creare un dizionare in cui ogni token viene associato al proprio valore TF moltiplicato per il corrispodente valore IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(tokens, idfs):\n",
    "    \"\"\" Compute TF-IDF\n",
    "    \n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "        idfs (dictionary): record to IDF value\n",
    "    \n",
    "    Returns:\n",
    "        dictionary: a dictionary of records to TF-IDF values\n",
    "    \"\"\"\n",
    "    tfs = tf(tokens)\n",
    "    tf_idf_dict = {t: tfs[t]*idfs[t] for t in tokens}\n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applichiamo la funzione `tfidf` al calcolo della misura TF.IDF per il prodotto di Amazon il cui ID vale `b000hkgj8k`. Tale operazione richieder&agrave; di estrarre il record corrispondente dal dataset di Amazon, nonchÃ© di convertire l'RDD che contiene i valori IFS per il corpus in un dizionario. Il primo compito si effettua facilmente applicando un'operazione di filtraggio, mentre per il secondo &egrave; possibile sfruttare l'azione `collectAsMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recb000hkgj8k = amazon_rec_to_token.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\n",
    "idfs_small_weights = idfs_small.collectAsMap()\n",
    "rec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfs_small_weights)\n",
    "\n",
    "print('Amazon record \"b000hkgj8k\" '\n",
    "      'has tokens and weights:\\n{}'.format(rec_b000hkgj8k_weights))\n",
    "\n",
    "assert(rec_b000hkgj8k_weights ==\n",
    "                   {'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,\n",
    "                    'courseware': 66.66666666666666, 'psg': 33.33333333333333,\n",
    "                    '2007': 3.5087719298245617, 'customizing': 16.666666666666664,\n",
    "                    'interface': 3.0303030303030303})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uso della distanza del coseno ##\n",
    "\n",
    "Una possibile distanza tra documenti Ã¨ la cosiddetta *distanza del coseno*, che interpreta due oggetti come direzioni in uno spazio e calcola il coseno dell'angolo che queste direzioni formano.\n",
    "\n",
    "La prima difficolt&agrave; consiste nel codificare i documenti come direzioni nello spazio, e quindi come vettori. In realt&agrave; avendo a disposizione la tokenizzazione dei nostri documenti, possiamo pensare a uno spazio che ha una dimensione per ogni possibile token del nostro corpus: un generico documento avrÃ  come componente nella dimensione che corrisponde a un token il corrispndente valore della misura TF.IDF (con l'ovvia estensione che se un token non occorre in un documento, allora la componente nella dimensione di tale token sarÃ  nulla).\n",
    "\n",
    "Il secondo problema da risolvere &egrave; quello di determinare l'angolo tra due vettori, ma ci&ograve; si pu&ograve; fare facilmente ricordando che \\\\( a \\cdot b = \\| a \\| \\| b \\| \\cos \\theta \\\\), dove \\\\( a \\cdot b \\\\) indica il prodotto scalare tra due vettori \\\\(a\\\\) e \\\\(b\\\\), \\\\(\\theta\\\\) l'angolo tra questi vettori e \\\\(\\| a \\|\\\\) denota la norma di \\\\(a\\\\). Pertanto\n",
    "\n",
    "$$ \\mathrm{sim}(a, b) = \\cos \\theta = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}} .$$\n",
    "\n",
    "Va tenuto presente che, sebbene procedendo in questo modo vi sia da considerare un elevato numero di dimensioni, i vettori possono essere memorizzati in un dizionario che contenga solo le componenti non nulle, sfruttando la propriet&agrave; di sparsit&agrave;. Pi&ugrave; precisamente, per ogni token \\\\(t\\\\) avente misura TF.IDF \\\\(i \\neq 0\\\\), in tale dizionario la chiave \\\\(t\\\\) sar&agrave; associata al valore \\\\(i\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implementare prodotto scalare e norma per vettori sparsi ###\n",
    "\n",
    "Usando le funzioni `tokenize` e `tfidf` e sfruttando la rappresentazione sparsa in dizionari &egrave; possibile:\n",
    "\n",
    "* definire una funzione `dotprod` che riceve come argomenti i dizionari che descrivono due vettori sparsi e che restituisce il corrispondente prodotto scalare (che in questo caso sar&agrave; uguale alla somma dei prodotti dei valori corrispondenti a chiavi che occorrono in *entrambi* i dizionari);\n",
    "* definire una funzione `norm` che restituisce la norma di un vettore sparso passato tramite il corrispondente dizionario (calcolandola come la radice quadrata del prodotto scalare del vettore con se stesso);\n",
    "* definire una funzione `cossim` that restituisca la distanza del coseno tra i due vettori sparsi specificati come argomenti, sfruttando la formula sopra riportata e le funzioni definite ai punti precedenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dotprod(a, b):\n",
    "    \"\"\" Compute dot product\n",
    "    \n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "        \n",
    "    Returns:\n",
    "        dotProd: result of the dot product with the two input dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum([a[t] * b[t] for t in a if t in b])\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product\n",
    "    \n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value\n",
    "        \n",
    "    Returns:\n",
    "        norm: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    return math.sqrt(dotprod(a, a))\n",
    "\n",
    "def cossim(a, b):\n",
    "    \"\"\" Compute cosine similarity\n",
    "    \n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "        \n",
    "    Returns:\n",
    "        cossim: dot product of two dictionaries divided by the norm of the first dictionary and\n",
    "                then by the norm of the second dictionary\n",
    "    \"\"\"\n",
    "    return dotprod(a, b)/(norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso, verifichiamo il comportamento di queste funzione tramite semplici test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
    "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
    "dp = dotprod(testVec1, testVec2)\n",
    "nm = norm(testVec1)\n",
    "print(dp, nm)\n",
    "assert(dp == 102)\n",
    "assert(abs(nm - 6.16441400297) < 0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim(testVec1, testVec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementare una funzione per il calcolo della similarit&agrave; ###\n",
    "\n",
    "Possiamo ora implementare una funzione `cosine_similarity` che, date due stringhe che descrivono documenti e un dizionario per i valori IDF in un corpus, calcola e restituisce la distanza del coseno tra i corrispondenti vettori nello spazio dei token. I passi necessari sono:\n",
    "\n",
    "* tokenizzare le due stringhe e successivamente utilizzarle unitamente al dizionario per invocare la funzione `tfidf`, e\n",
    "* invocare la funzione `cossim` e restituirne il risultato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(string1, string2, idfs_dictionary):\n",
    "    \"\"\" Compute cosine similarity between two strings\n",
    "    \n",
    "    Args:\n",
    "        string1 (str): first string\n",
    "        string2 (str): second string\n",
    "        idfsDictionary (dictionary): a dictionary of IDF values\n",
    "        \n",
    "    Returns:\n",
    "        cossim: cosine similarity value\n",
    "    \"\"\"\n",
    "    \n",
    "    w1 = tfidf(tokenize(string1), idfs_dictionary)\n",
    "    w2 = tfidf(tokenize(string2), idfs_dictionary)\n",
    "    return cossim(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&Egrave; quindi ora possibile verificare la similaritÃ  tra due prodottti, date le loro descrizioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossimAdobe = cosine_similarity('Adobe Photoshop',\n",
    "                               'Adobe Illustrator',\n",
    "                               idfs_small_weights)\n",
    "\n",
    "print(cossimAdobe)\n",
    "assert(abs(cossimAdobe - 0.0577243382163) < 0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Esecuzione della procedura di entity resolution ###\n",
    "\n",
    "Gli strumenti realizzati ci consentono ora di effettuare la procedura di entity resolution: per ogni prodotto nel dataset di Google possiamo calcolare la distanza del coseno rispetto a tutti i prodotti nel dataset di Amazon. In particolare procederemo come segue:\n",
    "\n",
    "* creeremo un RDD che contenga tutte le coppie di prodotti nei due dataset, ottenendo come generico elemento la tupla `((Google URL, Google String), (Amazon ID, Amazon String))`;\n",
    "* definiremo poi una funzione che accetti una siffatta tupla come argomento e restituisca la distanza del coseno tra i due prodotti ivi contenuti;\n",
    "* applicheremo infine tale funzione a tutti gli elementi del RDD creato al primo punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_small = (google_small\n",
    "              .cartesian(amazon_small)\n",
    "              .cache())\n",
    "\n",
    "def compute_similarity(record):\n",
    "    \"\"\" Compute similarity on a combination record\n",
    "    \n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record)\n",
    "        \n",
    "    Returns:\n",
    "        pair: a pair, (google URL, amazon ID, cosine similarity value)\n",
    "    \"\"\"\n",
    "    \n",
    "    google_rec = record[0]\n",
    "    amazon_rec = record[1]\n",
    "    google_URL = google_rec[0]\n",
    "    amazon_ID = amazon_rec[0]\n",
    "    google_value = google_rec[1]\n",
    "    amazon_value = amazon_rec[1]\n",
    "    \n",
    "    cs = cosine_similarity(google_value, amazon_value, idfs_small_weights)\n",
    "    return (google_URL, amazon_ID, cs)\n",
    "\n",
    "similarities = (cross_small\n",
    "                .map(lambda r: compute_similarity(r))\n",
    "                .cache())\n",
    "\n",
    "def similar(amazon_ID, google_URL):\n",
    "    \"\"\" Return similarity value\n",
    "    \n",
    "    Args:\n",
    "        amazon_ID: amazon ID\n",
    "        google_URL: google URL\n",
    "        \n",
    "    Returns:\n",
    "        similar: cosine similarity value\n",
    "    \"\"\"\n",
    "    \n",
    "    return (similarities\n",
    "            .filter(lambda record: (record[0] == google_URL and record[1] == amazon_ID))\n",
    "            .collect()[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamo ora in grado di visualizzare, per esempio, il grado di similarit&agrave; tra il prodotto di Amazon avente ID `b000o24l3q` e il prodotto di Google avente URL `http://www.google.com/base/feeds/snippets/17242822440574356561`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_Amazon_Google = \\\n",
    "      similar('b000o24l3q',\n",
    "              'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print('Requested similarity is {}.'.format(similarity_Amazon_Google))\n",
    "assert(abs(similarity_Amazon_Google - 0.000303171940451) < 0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il grado di similarit&agrave; &egrave; basso: verifichiamo che i documenti corrispondenti siano effettivamente diversi, visualizzando i primi \\\\(300\\\\) caratteri di entrambi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(amazon_rec_to_token.filter(lambda p: p[0]=='b000o24l3q')\n",
    "                            .take(1)[0][1])[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(google_rec_to_token.filter( \\\n",
    "    lambda p: p[0]=='http://www.google.com/base/feeds/snippets/17242822440574356561').take(1)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettivamete si trtta di due documenti differenti: probabilmente da un lato del software, dall'altro della musica. Le cose sono diverse per i documenti `b00004tkvy` e `http://www.google.com/base/feeds/snippets/18441110047404795849`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_Amazon_Google = similar('b00004tkvy', 'http://www.google.com/base/feeds/snippets/18441110047404795849')\n",
    "print('Requested similarity is {}.'.format(similarity_Amazon_Google))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso la similarit&agrave; &egrave; elevata, e infatti i documenti hanno in comune molti termini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(amazon_rec_to_token.filter(lambda p: p[0]=='b00004tkvy')\n",
    "                            .take(1)[0][1])[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(google_rec_to_token.filter(lambda p: p[0]=='http://www.google.com/base/feeds/snippets/18441110047404795849')\n",
    "                            .take(1)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Utilizzo di variabili broadcast ###\n",
    "\n",
    "La soluzione del Paragrafo 3.3 funziona efficacemente quando si utilizzano dataset di piccole dimensioni. Vi &egrave; per&ograve; un problema di efficienza nel momento in cui l'esecuzione della funzione `compute_similarity` viene distribuita tramite `map` su diversi nodi del cluster. In questo caso infatti il dizionario `idfs_small_weights` dovrebbe venire distribuito a tutti i nodi (pi&ugrave; volte se si invoca `similar` ripetutamente).\n",
    "\n",
    "In casi come questi ha senso utilizzare una funzionalit&agrave; specifica di Spark chiamata *variabile broadcast*, cio&egrave; una variabile definita nel programma driver a cui i worker possono fare riferimento: una siffatta variabile viene inviata una sola volta ai vari worker, che la salvano localmente.\n",
    "\n",
    "Iniziamo a definire una variabile broadcast per il dizionario `idfs_small_weights`, invocando la funzione `broadcast` sullo spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs_small_broadcast = sc.broadcast(idfs_small_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora &egrave; possibile definire una funzione `compute_similarity_broadcast`, analoga di `computeSimilarity` ma che utilizza la variabile broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_broadcast(record):\n",
    "    \"\"\" Compute similarity on a combination record, using a\n",
    "    Broadcast variable\n",
    "    \n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record)\n",
    "        \n",
    "    Returns:\n",
    "        a tuple (google URL, amazon ID, cosine similarity value)\n",
    "    \"\"\"\n",
    "    \n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    \n",
    "    googleURL = googleRec[0]\n",
    "    amazonID = amazonRec[0]\n",
    "    \n",
    "    googleValue = googleRec[1]\n",
    "    amazonValue = amazonRec[1]\n",
    "    \n",
    "    cs = cosine_similarity(googleValue, amazonValue,\n",
    "                          idfs_small_broadcast.value)\n",
    "    return (googleURL, amazonID, cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedendo come nella parte precedente possiamo applicare questa funzione a tutte le coppie di prodotti e successivamente definire una funzione che filtri i record ottenuti al fine di valutare la similarit&agrave; tra due specifici documenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_broadcast = (cross_small\n",
    "                         .map(lambda r: compute_similarity_broadcast(r))\n",
    "                         .cache())\n",
    "\n",
    "def similar_broadcast(amazon_ID, google_URL):\n",
    "    \"\"\" Return similarity value, computed using Broadcast variable\n",
    "    \n",
    "    Args:\n",
    "        amazon_ID: amazon ID\n",
    "        google_URL: google URL\n",
    "        \n",
    "    Returns:\n",
    "        similar: cosine similarity value\n",
    "    \"\"\"\n",
    "    return (similarities_broadcast\n",
    "            .filter(lambda record: (record[0] == google_URL and record[1] == amazon_ID))\n",
    "            .collect()[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per verificare che tutto sia andato a buon fine, ricalcoliamo la similarit&agrave; tra il documento `b000o24l3q` e il documento `http://www.google.com/base/feeds/snippets/17242822440574356561`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_amazon_google_broadcast = similar_broadcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print('Requested similarity is {}.'.format(similarity_amazon_google_broadcast))\n",
    "\n",
    "from pyspark import Broadcast\n",
    "assert(isinstance(idfs_small_broadcast, Broadcast))\n",
    "assert(len(idfs_small_broadcast.value) == 4772)\n",
    "assert(abs(similarity_amazon_google_broadcast - 0.000303171940451) < 0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Valutare le performance ###\n",
    "\n",
    "Il file `Amazon_Google_perfectMapping.csv` contiene un insieme di corrispondenze tra prodotti nei due dataset che si sanno essere simili. Questo tipo di informazione, cui si fa riferimento utilizzando i termini *ground truth* o *gold standard*, pu&ograve; essere utilizzata per valutare la bontÃ  della procedura di entity resolution.\n",
    "\n",
    "Iniziamo leggendo il file contenente la ground truth e creiamo il corrispondente RDD, i cui elementi avranno come formato `(\"AmazonID GoogleURL\", 'gold')`. Utilizzeremo a tal scopo una funzione il cui comportamento &egrave; simile alla funzione `parse_datafile_line` utilizzata nel Paragrafo 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDFILE_PATTERN = '^(.+),(.+)'\n",
    "\n",
    "def parse_goldfile_line(goldfile_line):\n",
    "    \"\"\" Parse a line from the 'golden standard' data file\n",
    "    \n",
    "    Args:\n",
    "        goldfile_line: a line of data\n",
    "        \n",
    "    Returns:\n",
    "        pair: ((key, 'gold'), 1) if successful or else\n",
    "        (goldfile_line, 0) when fed with the heading\n",
    "        line and (goldfile_line, -1) when a parsing error\n",
    "        occurs\n",
    "    \"\"\"\n",
    "    \n",
    "    match = re.search(GOLDFILE_PATTERN, goldfile_line)\n",
    "    \n",
    "    if match is None:\n",
    "        print('Invalid goldfile line: {}'.format(goldfile_line))\n",
    "        return (goldfile_line, -1)\n",
    "    \n",
    "    elif match.group(1) == '\"idAmazon\"':\n",
    "        print('Header datafile line: {}'.format(goldfile_line))\n",
    "        return (goldfile_line, 0)\n",
    "    \n",
    "    else:\n",
    "        key = '%s %s' % (remove_quotes(match.group(1)),\n",
    "                         remove_quotes(match.group(2)))\n",
    "        return ((key, 'gold'), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando lo stesso approccio utilizzato per leggere i dataset campionati Ã¨ possibile mappare la fuzione `parse_goldfile_line` su tutte le linee del file contenente la ground truth e successivamente estrarre dai risultati il RDD relativo a eventuali errori di parsing e quello contenente i dati convertiti, verificando successivamente che non sia avvenuto alcun errore di parsing, e che tutti i dati siano stati considerati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldfile = os.path.join(base_dir, GOLD_STANDARD_PATH)\n",
    "\n",
    "gs_raw = (sc\n",
    "         .textFile(goldfile)\n",
    "         .map(parse_goldfile_line)\n",
    "         .cache())\n",
    "\n",
    "gs_failed = (gs_raw\n",
    "            .filter(lambda s: s[1] == -1)\n",
    "            .map(lambda s: s[0]))\n",
    "\n",
    "for line in gs_failed.take(10):\n",
    "    print('Invalid goldfile line: {}'.format(line))\n",
    "\n",
    "gold_standard = (gs_raw\n",
    "                .filter(lambda s: s[1] == 1)\n",
    "                .map(lambda s: s[0])\n",
    "                .cache())\n",
    "\n",
    "print('Read {} lines, successfully parsed {} lines, '\n",
    "      'failed to parse {} lines'.format(gs_raw.count(),\n",
    "                                        gold_standard.count(),\n",
    "                                        gs_failed.count()))\n",
    "\n",
    "assert (gs_failed.count() == 0)\n",
    "assert (gs_raw.count() == (gold_standard.count() + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizzando la *ground truth* possiamo capire quante coppie di oggetti nei dataset campionati siano effettivamente simili e calcolare il corrispondente valor medio per l'indice di similarit&agrave;.\n",
    "\n",
    "Per fare questo, iniziamo creando un RDD `sims` ottenuto da `similarities_broadcast`, in cui ogni elemento ha come formato `(\"AmazonID GoogleURL\", cosineSimilarityScore)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = similarities_broadcast.map(lambda r: (r[1] + ' ' + r[0], r[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuando un'operazione di join tra gli RDD `sims` e `gold_standard` Ã¨ possibile costruire un RDD `true_dups_RDD` che contiene i valori di similarit&agrave; per le coppie di documenti che abbiamo analizzato e che fanno parte della *ground truth*. Da questo nuovo RDD &egrave; immediato ottenere il numero di tali coppie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dups_RDD = (sims\n",
    "               .join(gold_standard))\n",
    "\n",
    "true_dups_count = true_dups_RDD.count()\n",
    "\n",
    "print('There are {} true duplicates.'.format(true_dups_count))\n",
    "assert(true_dups_count == 146)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo consiste nel calcolare il valore medio dell'indice di similarit&agrave; per le coppie di documenti che sappiamo essere simili, estraendolo opportunamente da `true_dups_RDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim_dups = float(sum(true_dups_RDD.map(lambda s: s[1][0])\n",
    "                         .collect())) / true_dups_count\n",
    "\n",
    "print('The average similarity of true duplicates is {}.'.format(avg_sim_dups))\n",
    "assert(abs(avg_sim_dups - 0.264332573435) < 0.0000001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo calcolare la media dell'indice di similarit&agrave; anche per le coppie di documenti nei dataset campionati che invece sappiamo **non** essere simili (in quanto le coppie corrispondenti non sono elencate nel file contenente la *ground truth*). Il RDD che contiene gli identificativi di tali coppie e la corrispondente similarit&agrave; si ottiene facilmente considerando la differenza simmetrica tra i RDD `sims` e `gold_standard`, utilizzando la funzione `subtractByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dups_RDD = (sims\n",
    "              .subtractByKey(gold_standard))\n",
    "\n",
    "avg_sim_non = float(sum(non_dups_RDD.map(lambda s: s[1])\n",
    "                        .collect())) / non_dups_RDD.count()\n",
    "\n",
    "print('And for non duplicates, it is {}.'.format(avg_sim_non))\n",
    "assert(abs(avg_sim_non - 0.00123476304656) < 0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entity resolution scalabile ##\n",
    "\n",
    "Nei paragrafi precedenti abbiamo costruito una funzione per il calcolo della similarit&agrave; tra testi e l'abbiamo utilizzata per effettuare della entity resolution su piccola scala. L'implementazione soffre di una complessit&agrave; temporale quadratica, cosa che la rende inefficiente anche per dataset di dimensione modesta. In questo paragrafo implementeremo un algoritmo pi&ugrave; efficiente, al fine di applicarlo all'intero dataset.\n",
    "\n",
    "Per aumentare l'efficienza dell'algoritmo usato finora &egrave; opportuno analizzarne la complessit&agrave; temporale. In particolare, questo algoritmo &egrave; quadratico per due motivi. Innanzitutto abbiamo eseguito in modo ridondante molti calcoli quando ci siamo occupati di token e pesi, in quanto ogni record veniva ricalcolato ogni volta che effettuavamo un confonto. Inoltre, abbiamo effettuato un numero quadratico di confronti tra record.\n",
    "\n",
    "La prima fonte di complessit&agrave; quadratica puÃ² essere eliminata precalcolando i valori e memorizzandoli, ma la seconda fonte &egrave; pi&ugrave; difficile da trattare. Nel caso peggiore, ogni token in ogni record di un dataset esister&agrave; in ogni record dell'altro dataset, e quindi ogni token avr&agrave; un contributo non nullo nel calcolo del coefficiente di similarit&agrave;. In realt&agrave; la maggior parte dei record hanno molto poco (se non nulla) in comune. Inoltre &egrave; tipico che ogni record in un dataset abbia al massimo un duplicato nel rimanente dataset. In casi come questo l'output &egrave; lineare nella dimensione dell'input e si pu&ograve; sperare di avere anche un tempo di esecuzione che sia lineare.\n",
    "\n",
    "La struttura dati che ci pemetter&agrave; di effettuare un numero quadratico di confronti si chiama *indice inverso*, e mappa ogni token nel dataset nella lista dei documenti che lo contengono. Utilizzando questa struttura dati, invece di confrontare due record token per token, &egrave; possibile semplicemente *cercare* quali record contengano un token specifico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Estrazione dei token ###\n",
    "\n",
    "Iniziamo estraendo i token dai dataset completi di Google e Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_full_rec_to_token = amazon.map(lambda s: (s[0], tokenize(s[1])))\n",
    "google_full_rec_to_token = google.map(lambda s: (s[0], tokenize(s[1])))\n",
    "print('Amazon full dataset is {} products, '\n",
    "      'Google full dataset is {} '\n",
    "      'products'.format(amazon_full_rec_to_token.count(),\n",
    "                        google_full_rec_to_token.count()))\n",
    "\n",
    "assert(amazon_full_rec_to_token.count() == 1363)\n",
    "assert(google_full_rec_to_token.count() == 3226)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calcolo della misura TF-IDF ###\n",
    "\n",
    "Possiamo riutilizzare il codice giÃ  scritto per calcolare i valori IDF per i dataset completi. Iniziamo creando un RDD `full_corpus_RDD` che unisca i due dataset, e applichiamo a questo RDD la funzione `idfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus_RDD = amazon_full_rec_to_token.union(google_full_rec_to_token)\n",
    "idfs_full = idfs(full_corpus_RDD)\n",
    "idfs_full_count = idfs_full.count()\n",
    "print('There are {} unique tokens '\n",
    "      'in the full datasets.'.format(idfs_full_count))\n",
    "assert(idfs_full_count == 17078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si continua sempre in modo analogo, trasformando il RDD con i valori IDF in un dizionario e generando una variabile broadcast a partire da quest'ultimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute IDFs for full dataset\n",
    "idfs_full_weights = idfs_full.collectAsMap()\n",
    "idfs_full_broadcast = sc.broadcast(idfs_full_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo consiste nel riutilizzare la funzione `tfidfs` assieme alla variabile broadcast, al fine di ottenere dei RDD che associno gli identificativi dei prodotti alle relativo dizionario che mappa i token alla corrispondente misura TF.IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.\n",
    "amazon_weights_RDD = amazon_full_rec_to_token.map(\n",
    "    lambda s: (s[0], tfidf(s[1], idfs_full_broadcast.value)))\n",
    "google_weights_RDD = google_full_rec_to_token.map(\n",
    "    lambda s: (s[0], tfidf(s[1], idfs_full_broadcast.value)))\n",
    "\n",
    "print('There are {} Amazon weights '\n",
    "      'and {} Google weights.'.format(amazon_weights_RDD.count(),\n",
    "                                      google_weights_RDD.count()))\n",
    "    \n",
    "assert(amazon_weights_RDD.count() == 1363)\n",
    "assert(google_weights_RDD.count() == 3226)                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calcolo delle norme per i vettori delle misure TF.IDF ###\n",
    "\n",
    "Per implementare in modo efficiente la procedura di entity resolution nei dataset completi, creiamo due RDD i cui elementi sono coppie che contengono l'identificativo di un documento e la norma del corrispondente vettore di misure TF.IDF. Tali RDD vanno poi convertiti in dizionari e utilizzati per ottenere delle variabili broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_norms = amazon_weights_RDD.map(lambda s: (s[0], norm(s[1])))\n",
    "amazon_norms_broadcast = sc.broadcast(amazon_norms.collectAsMap())\n",
    "\n",
    "google_norms = google_weights_RDD.map(lambda s: (s[0], norm(s[1])))\n",
    "google_norms_broadcast = sc.broadcast(google_norms.collectAsMap())\n",
    "\n",
    "assert(isinstance(amazon_norms_broadcast, Broadcast))\n",
    "assert(len(amazon_norms_broadcast.value) == 1363)\n",
    "assert(isinstance(google_norms_broadcast, Broadcast))\n",
    "assert(len(google_norms_broadcast.value) == 3226)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Creazione di indici inversi per i dataset completi ###\n",
    "\n",
    "Per creare gli indici inversi &egrave; necessario innanzitutto scrivere una funzione `invert` che data una coppia `(id, vettore di valori TF.IDF)` restituisce una lista di coppie `(token, id)`, ricordando che il vettore dei valori TF.IDF &egrave; memorizzato in modo sparso come dizionario che mappa i token ai valori.\n",
    "\n",
    "La funzione `invert` pu&ograve; quindi essere utilizzata per convertire i dataset completi nei corrispondenti indici inversi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(record):\n",
    "    \"\"\" Invert (ID, tokens) to a list of (token, ID)\n",
    "    \n",
    "    Args:\n",
    "        record: a pair, (ID, token vector)\n",
    "        \n",
    "    Returns:\n",
    "        pairs: a list of pairs of token to ID\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = [(f, record[0]) for f in record[1]]\n",
    "    return pairs\n",
    "\n",
    "amazon_inv_pairs_RDD = (amazon_weights_RDD\n",
    "                    .flatMap(lambda r: invert(r))\n",
    "                    .cache())\n",
    "\n",
    "google_inv_pairs_RDD = (google_weights_RDD\n",
    "                    .flatMap(lambda r: invert(r))\n",
    "                    .cache())\n",
    "\n",
    "print('There are {} Amazon inverted pairs '\n",
    "      'and {} Google inverted pairs.'.format(amazon_inv_pairs_RDD.count(),\n",
    "                                             google_inv_pairs_RDD.count()))\n",
    "\n",
    "inverted_pair = invert((1, {'foo': 2}))\n",
    "assert(inverted_pair[0][1] == 1)\n",
    "assert(amazon_inv_pairs_RDD.count() == 111386)\n",
    "assert(google_inv_pairs_RDD.count() == 77678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Identificare token in comune nei dataset completi ##\n",
    "\n",
    "&Egrave; ora possibile eseguire in modo efficiente la procedura di entity resolution sui dataset completi. L'operazione iniziale consiste nel costruire un RDD che ha come chiave una coppia `(ID, URL)` e come corrispondente valore la lista di token comuni nelle descrizioni dei corrispondenti prodotti. Per fare questo &egrave; necessario:\n",
    "\n",
    "* utilizzare i due indici inversi (RDD i cui elementi sono coppie che come primo elemento hanno un token e come secondo elemento l'ID o l'URL di un documento che contiene tale token) per creare un nuovo RDD che si riferisce solo ai token contenuti in entrambi i dataset, e il cui generico elemento &egrave; una coppia avente in prima posiizone un token e in seconda un iteratore di coppie `(ID, URL)`;\n",
    "* creare un nuovo RDD ottenuto invertendo gli elementi delle coppie ottenute al punto precedente;\n",
    "* creare un RDD che permetta di mappare le coppie `(ID, URL)` nella lista dei token comuni corrispondenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(record):\n",
    "    \"\"\" Swap (token, (ID, URL)) to ((ID, URL), token)\n",
    "    \n",
    "    Args:\n",
    "        record: a pair, (token, (ID, URL))\n",
    "        \n",
    "    Returns:\n",
    "        pair: ((ID, URL), token)\n",
    "    \"\"\"\n",
    "    token = record[0]\n",
    "    keys = record[1]\n",
    "    return (keys, token)\n",
    "\n",
    "common_tokens = (amazon_inv_pairs_RDD\n",
    "                .join(google_inv_pairs_RDD)\n",
    "                .map(lambda r: swap(r))\n",
    "                .groupByKey()\n",
    "                .cache())\n",
    "\n",
    "print('Found {} common tokens'.format(common_tokens.count()))\n",
    "\n",
    "assert(common_tokens.count() == 2441100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto &egrave; possibile utilizzare le strutture dati create per costruire un dizionario che trasformi le coppie `(ID, URL)` nella corrispondenza misura di similarit&agrave; basata sulla distanza del coseno, eseguendo i passi che seguono.\n",
    "\n",
    "* Creazione di due dizionari broadcast a partire dai RDD `amazon_weights_RDD` e `google_weights_RDD`;\n",
    "* Implementazione di un afunzione `fast_cosine_similarity` che accetta come argomento un record ((Amazon ID, Google URL), tokens list) e calcola il prodotto scalare corrispondente per poi dividerlo per la norma dei vettori che descrivono i due documenti coinvolti; la funzione dovr&agrave; restituire una coppia che ha il risultato ottenuto come valore e la coppia `(Amazon ID, Google URL)` come chiave;\n",
    "* Applicazione di `fast_cosines_similarity` ai token comuni ai due dataset completi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_weights_broadcast = sc.broadcast( \\\n",
    "                        amazon_weights_RDD.collectAsMap())\n",
    "google_weights_broadcast = sc.broadcast( \\\n",
    "                        google_weights_RDD.collectAsMap())\n",
    "\n",
    "def fast_cosine_similarity(record):\n",
    "    \"\"\" Compute Cosine Similarity using Broadcast variables\n",
    "    \n",
    "    Args:\n",
    "        record: ((ID, URL), token)\n",
    "        \n",
    "    Returns:\n",
    "        pair: ((ID, URL), cosine similarity value)\n",
    "    \"\"\"\n",
    "    \n",
    "    amazon_rec = record[0][0]\n",
    "    google_rec = record[0][1]\n",
    "    tokens = record[1]\n",
    "    s = sum([amazon_weights_broadcast.value[amazon_rec][t] * \\\n",
    "             google_weights_broadcast.value[google_rec][t]\n",
    "             for t in tokens])\n",
    "    value = s / (amazon_norms_broadcast.value[amazon_rec] * \\\n",
    "                 google_norms_broadcast.value[google_rec])\n",
    "    key = (amazon_rec, google_rec)\n",
    "    return (key, value)\n",
    "\n",
    "similarities_full_RDD = (common_tokens\n",
    "                    .map(lambda r: fast_cosine_similarity(r))\n",
    "                    .cache())\n",
    "\n",
    "print(similarities_full_RDD.count())\n",
    "\n",
    "g_URL = 'http://www.google.com/base/feeds/snippets/13823221823254120257'\n",
    "\n",
    "item_filter = lambda id_url_cs: id_url_cs[0][0] == 'b00005lzly' \\\n",
    "                                and id_url_cs[0][1] == g_URL\n",
    "\n",
    "similarity_test = similarities_full_RDD.filter(item_filter).collect()\n",
    "assert(len(similarity_test) == 1)\n",
    "assert(abs(similarity_test[0][1] - 4.286548414e-06) < 0.000000000001)\n",
    "assert(similarities_full_RDD.count() == 2441100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analisi ##\n",
    "\n",
    "Ora che abbiamo un modo di associare a una coppia di documenti la similarit&agrave; corrispondente, dobbiamo stabilire come utilizzare questa informazione per decidere quando una coppia di prodotti individua un duplicato. Il modo pi&ugrave; semplice di procedere &egrave; quello di utilizzare una soglia: le coppie di record con un valore di similarit&agrave; superiore verranno segnalate come duplicati, mentre i prodotti in quelle rimanenti saranno dichiarati come distinti.\n",
    "\n",
    "Per decidere a quale valore impostare questa soglia &egrave; necessario realizzare come essa influenzi tipi di errori differenti. Se la soglia &egrave; troppo bassa si otterranno infatti dei cosiddetti *falsi positivi*, che corrispondono a record dichiarati duplicati ma che nella realt&agrave; sono distinti. Dualmente, se la soglia &egrave; troppo alta aumenter&agrave; il numero di *falsi negativi*, intesi come record duplicati che non vengono rilevati.\n",
    "\n",
    "Valuteremo la procedura di entity resolution utilizzando due metriche tipiche del campo dell'information retrieval, chiamate *precision* e *recall*:\n",
    "\n",
    "* il valore di *precision* indica qual &egrave; la frazione dei record dichiarati come duplicati che lo sono effettivamente, mentre\n",
    "\n",
    "* il valore di *recall* indica qual &egrave; la frazione di record duplicati che vengono effettivamente rilevati.\n",
    "\n",
    "Siccome queste due metriche tendono ad assumere valori opposti, per misurare la bontÃ  si utilizza una terza metrica detta *F-measure*, ottenuta calcolando la media armonica di *precision* e *recall*. Indicando rispettivamente con \\\\(p\\\\), \\\\(r\\\\) e \\\\(F\\\\) i valori di *precision*, *recall* e di questa nuova metrica si ha cio&egrave;\n",
    "\n",
    "$$ F = 2 \\frac{p \\cdot r}{p + r} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Conteggio di veri positivi, falsi positivi e falsi negativi ##\n",
    "\n",
    "Per calcolare il numero di coppie duplicate dobbiamo:\n",
    "\n",
    "* creare `sims_full_RDD` a partire da `similarities_full_RDD`, in modo da considerare solo le coppie (ID, URL) e i valori di similarit&agrave;;\n",
    "* estrarre da questo RDD i soli valori di similarit&agrave;.\n",
    "\n",
    "Per ottenere i valori di similarit&agrave; per i le coppie duplicate baster&agrave; effettuare una left outer join tra `gold_standard` e `sims_full_RDD` per poi estrarre la seconda componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of ((Amazon ID, Google URL), similarity score)\n",
    "sims_full_RDD = similarities_full_RDD.map( \\\n",
    "                    lambda x: ('{} {}'.format(x[0][0], x[0][1]), x[1]))\n",
    "assert (sims_full_RDD.count() == 2441100)\n",
    "\n",
    "# Create an RDD of just the similarity scores\n",
    "sims_full_values_RDD = (sims_full_RDD\n",
    "                     .map(lambda x: x[1])\n",
    "                     .cache())\n",
    "\n",
    "assert(sims_full_values_RDD.count() == 2441100)\n",
    "\n",
    "# Look up all similarity scores for true duplicates\n",
    "\n",
    "# This helper function will return the similarity score\n",
    "# for records that are in the gold standard and the simsFullRDD\n",
    "# (True positives), and will return 0 for records that are\n",
    "#in the gold standard but not in simsFullRDD (False Negatives).\n",
    "\n",
    "def gs_value(record):\n",
    "    if (record[1][1] is None):\n",
    "        return 0\n",
    "    else:\n",
    "        return record[1][1]\n",
    "\n",
    "# Join the gold standard and simsFullRDD, and then extract\n",
    "# the similarities scores using the helper function\n",
    "\n",
    "true_dup_sims_RDD = (gold_standard\n",
    "                  .leftOuterJoin(sims_full_RDD)\n",
    "                  .map(gs_value)\n",
    "                  .cache())\n",
    "print('There are {} true duplicates.'.format(true_dup_sims_RDD.count()))\n",
    "assert(true_dup_sims_RDD.count() == 1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo consiste nel fissare una soglia tra \\\\(0\\\\) e \\\\(1\\\\) per il conteggio dei veri positivi (coppie duplicate il cui valore di similarit&agrave; &egrave; superiore alla soglia). Vogliamo per&ograve; sperimentare con differenti valori per la soglia, e per fare questo dividiamo lo spazio \\\\([0, 1]\\\\) in cui pu&ograve; variare la soglia in \\\\(100\\\\) intervalli e procediamo come segue:\n",
    "\n",
    "* utilizziamo gli *accumulatori* di Spark per implementare una funizone di conteggio; in particolare definiamo il tipo `VectorAccumulatorParam`, che comprende funzioni per inizializzare a zero il valore di un vettore di accumulatori e per addizionare due vettori; va notato come potremo utilizzare solo l'operatore `+=` in quanto &egrave; solo possibile aggiungere valori a un accumulatore;\n",
    "* scriviamo una funzione che crei una lista con un unico valore non nullo;\n",
    "* creiamo \\\\(101\\\\) contenitori per i possibili valori della soglia tra \\\\(0\\\\) e \\\\(1\\\\);\n",
    "* per ogni punteggio di similarit&agrave; possiamo calcolare il numero di falsi positivi, sommando i valori di similarit&agrave; al contenitore appropriato, per poi eliminare i veri positivi utilizzando i dati nel gold standard;\n",
    "* definiamo in questo modo funzioni per calcolare il numero di falsi positivi, falsi negativi e veri positivi per un dato valore della soglia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    # Initialize the VectorAccumulator to 0\n",
    "    def zero(self, value):\n",
    "        return [0] * len(value)\n",
    "\n",
    "    # Add two VectorAccumulator variables\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in range(len(val1)):\n",
    "            val1[i] += val2[i]\n",
    "        return val1\n",
    "\n",
    "# Return a list with entry x set to value\n",
    "# and all other entries set to 0\n",
    "def set_bit(x, value, length):\n",
    "    bits = []\n",
    "    for y in range(length):\n",
    "        if (x == y):\n",
    "            bits.append(value)\n",
    "        else:\n",
    "            bits.append(0)\n",
    "    return bits\n",
    "\n",
    "# Pre-bin counts of false positives\n",
    "# for different threshold ranges\n",
    "\n",
    "BINS = 101\n",
    "nthresholds = 100\n",
    "\n",
    "def bin(similarity):\n",
    "    return int(similarity * nthresholds)\n",
    "\n",
    "# fpCounts[i] = number of entries (possible false positives)\n",
    "# where bin(similarity) == i\n",
    "\n",
    "zeros = [0] * BINS\n",
    "fpCounts = sc.accumulator(zeros, VectorAccumulatorParam())\n",
    "\n",
    "def add_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, 1, BINS)\n",
    "\n",
    "sims_full_values_RDD.foreach(add_element)\n",
    "\n",
    "# Remove true positives from FP counts\n",
    "def sub_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, -1, BINS)\n",
    "\n",
    "true_dup_sims_RDD.foreach(sub_element)\n",
    "\n",
    "def falsepos(threshold):\n",
    "    fpList = fpCounts.value\n",
    "    return sum([fpList[b] for b in range(0, BINS)\n",
    "                if float(b) / nthresholds >= threshold])\n",
    "\n",
    "def falseneg(threshold):\n",
    "    return true_dup_sims_RDD.filter( \\\n",
    "                lambda x: x < threshold).count()\n",
    "\n",
    "def truepos(threshold):\n",
    "    return true_dup_sims_RDD.count() - falsenegDict[threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Precision, Recall, e F-measure ###\n",
    "\n",
    "Possiamo ora definire delle funzioni che ci permettono di calcolare i valori di precision, recall e F-measure in funzione del valore di soglia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision = true-positives / (true-positives + false-positives)\n",
    "# Recall = true-positives / (true-positives + false-negatives)\n",
    "# F-measure = 2 x Recall x Precision / (Recall + Precision)\n",
    "\n",
    "def precision(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falseposDict[threshold])\n",
    "\n",
    "def recall(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falsenegDict[threshold])\n",
    "\n",
    "def fmeasure(threshold):\n",
    "    r = recall(threshold)\n",
    "    p = precision(threshold)\n",
    "    return 2 * r * p / (r + p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Grafici di precision, recall e F-measure ###\n",
    "\n",
    "Utilizzando le funzioni definite nel paragrafo precedente &egrave; possibile visualizzare l'andamento di precision, recall e F-measure in funzione del valore della soglia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]\n",
    "falseposDict = dict([(t, falsepos(t)) for t in thresholds])\n",
    "falsenegDict = dict([(t, falseneg(t)) for t in thresholds])\n",
    "trueposDict = dict([(t, truepos(t)) for t in thresholds])\n",
    "\n",
    "precisions = [precision(t) for t in thresholds]\n",
    "recalls = [recall(t) for t in thresholds]\n",
    "fmeasures = [fmeasure(t) for t in thresholds]\n",
    "\n",
    "print(precisions[0], fmeasures[0])\n",
    "assert (abs(precisions[0] - 0.000532546802671) < 0.0000001)\n",
    "assert (abs(fmeasures[0] - 0.00106452669505) < 0.0000001)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(thresholds, precisions)\n",
    "plt.plot(thresholds, recalls)\n",
    "plt.plot(thresholds, fmeasures)\n",
    "plt.legend(['Precision', 'Recall', 'F-measure'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "name": "entity-resolution-tutorial",
  "notebookId": 463537072256233
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
