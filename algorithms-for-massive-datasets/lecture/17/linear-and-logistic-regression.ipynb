{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "subslide_end",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/logo-di.jpg\" style=\"height: 4rem; margin-top: 1.1rem; float: left;\">\n",
    "<img src=\"img/logo-unimi.jpg\" style=\"height: 3.5rem; float: left; margin-right: 2em;\">\n",
    "\n",
    "<h3 syle=\"float: left;\">Master in Computer Science</h3>\n",
    "\n",
    "\n",
    "<p style=\"clear: both;\" />\n",
    "<h2>Algorithms for massive datasets</h2>\n",
    "\n",
    "<h1>Regression and logistic regression at big scale</h1>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<address>Dario Malchiodi<br/>\n",
    "malchiodi@di.unimi.it</address>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# It's (also) a machine learning problem\n",
    "\n",
    "* Data: a set $\\{ (x^{(1)}, y^{(1)}), \\dots, (x^{(n)}, y^{(n)}) \\}$ of associations between *objects* and *labels*\n",
    "* Goal: find a mapping from objects to labels\n",
    "  - describing observed data within a reasonable approximation level\n",
    "  - generalizing to unseen observations\n",
    "* Technically, a supervised learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation\n",
    "\n",
    "> Pyro: That's a dorky looking helmet. What's it for?\n",
    ">\n",
    "> Magneto: This Â«dorky looking helmetÂ» is the only thing that's going to protect me from the REAL bad guys.\n",
    "\n",
    "A good notation is like Magneto's helmet\n",
    "\n",
    "* $x^{(j)}$ denotes $j$-th object/vector in a series ($y^{(j)}$ the $j$-th label)\n",
    "* (to avoid confusion with exponentiation)\n",
    "* $x_i$ denotes $i$-th component of vector $x$\n",
    "* (mix and match: $x^{(j)}_i$ denotes $i$-th component of $j$-th vector)\n",
    "\n",
    "Will try to be consistent in using $i$ as component index and $j$ as object/vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression\n",
    "\n",
    "* generic object: $x \\in \\mathbb R^d$ for fixed $d \\in \\mathbb N$\n",
    "* generic label: $y \\in \\mathbb R$\n",
    "* approximation of label: $\\hat y \\in \\mathbb R$\n",
    "\n",
    "Assume a *linear* mapping between objects and labels\n",
    "\n",
    "$$ \\hat y = w \\cdot x = \\sum_{i=1}^d w_i x_i $$\n",
    "\n",
    "* $\\hat y$ is an approximation (or a prediction) for $y$\n",
    "* Our problem lies in finding $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Affine mapping integration\n",
    "\n",
    "* Adding a threshold/offset in the mapping may help a lot\n",
    "* Technically, this brings us to an affine mapping\n",
    "* Practically, just pretend you have an additional dimension and set its component to 1\n",
    "\n",
    "$$ x = (x_1, \\dots, x_d) \\rightarrow x = (1, x_1, \\dots, x_d) $$\n",
    "\n",
    "$$ \\hat y = w \\cdot x = \\sum_{i=0}^d w_i x_i = w_0 + w_1 x_1 + \\dots + w_d x_d $$\n",
    "\n",
    "Nothing changed in our problem (still in search for $w$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Uh, rather simple?\n",
    "\n",
    "* A feature!\n",
    "* An option not to be underestimated\n",
    "* Complexity injectable through feature extraction:\n",
    "  - augment object vectors adding product of components\n",
    "  - for instance, pairs of components capture covariance\n",
    "  - extendible to higher order moments\n",
    "\n",
    "For instance: $x = (x_1, x_2) \\rightarrow \\Phi(x) = (x_1^2, x_1 x_2, x_2 x_1, x_2^2)$\n",
    "\n",
    "Let's be clever: $\\Phi'(x) = (x_1^2, \\sqrt{2} x_1 x_2, x_2^2)$, because\n",
    "\n",
    "\\begin{align}\n",
    "\\Phi'(a) \\cdot \\Phi'(b) &= a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 + a_2^2 b_2^2 \\\\\n",
    "                        &= \\Phi(a) \\cdot \\Phi(b)\n",
    "\\end{align}\n",
    "\n",
    "Besides,\n",
    "\n",
    "$$ \\Phi'(a) \\cdot \\Phi'(b) = (a_1 b_1 + a_2 b_2)^2 = (a \\cdot b)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do I find $w$?\n",
    "\n",
    "* Pretend we have a candidate $w$\n",
    "* Let's measure how $w$ is good at prediction:\n",
    "  - $y$ is my label\n",
    "  - $\\hat y$ is the prediction\n",
    "  - need a loss function: squared error $(\\hat y - y)^2$\n",
    "* Let's cumulate errors on all observations:\n",
    "\n",
    "$$ \\ell(w) = \\sum_{j=1}^n \\left( \\hat y^{(j)} - y^{(j)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "Find $w$ minimizing cumulated loss:\n",
    "\n",
    "\\begin{align}\n",
    "w &= \\arg \\min_w \\ell(w) \\\\\n",
    "  &= \\arg \\min_w \\sum_{j=1}^n \\left( \\hat y^{(j)} - y^{(j)} \\right)^2 \\\\\n",
    "  &= \\arg \\min_w \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "$$ w = \\arg \\min_w \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2 $$\n",
    "\n",
    "- gather objects in the $n \\times d$ matrix $X$\n",
    "- gather labels in vector $y \\in \\mathbb R^n$\n",
    "\n",
    "$$ w = \\arg \\min_w || X w - y ||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "It's a convex problem, just nullify first derivatives:\n",
    "\n",
    "$$ \\frac{\\partial \\ell}{\\partial w} = 2 X^T (X w - y)  = 0 $$\n",
    "\n",
    "This brings to\n",
    "\n",
    "\\begin{align}\n",
    "X^T X w - X^T y &= 0 \\\\\n",
    "w = \\left( X^T X \\right)^{-1} X^T y\n",
    "\\end{align}\n",
    "\n",
    "(remember: $X$ is $n \\times d$, $X^T$ is $d \\times n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Remember generalization?\n",
    "\n",
    "* Real-world data is *dirty*\n",
    "* Aiming at the smallest loss could lead to *overfitting*\n",
    "* Occam's razor: find the right balance between model complexity and error\n",
    "* For instance: *Ridge regression*\n",
    "\n",
    "$$ w = \\arg \\min_w || X w - y ||_2^2 + \\lambda || w ||_2^2 $$\n",
    "\n",
    "* Closed form solution\n",
    "\n",
    "$$ w = \\left( X^T X + \\lambda I_d \\right)^{-1} X^T y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wait: $\\lambda$?\n",
    "\n",
    "* Hyper-parameter to be tuned\n",
    "* How can it be selected?\n",
    "* And what about assessing the learnt model capabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use data against overfitting\n",
    "\n",
    "* Split observations in three sets:\n",
    "  - *Training set*, used to train models (in our case: finding out $w$)\n",
    "  - *Validation set*, used for model selection (in our case: tuning $\\lambda$)\n",
    "  - *Test set*, used to assess the machine learning output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assessment\n",
    "\n",
    "* Fix an error measure, typically MSE\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{j=1}^n \\left( \\hat y^{(j)} - y^{(j)} \\right)^2 $$\n",
    "\n",
    "* or $ \\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning pipeline\n",
    "\n",
    "1. Fix a discretization $\\{ \\lambda_1, \\dots, \\lambda_o \\}$ of the parameter space\n",
    "2. For each discretized value $\\lambda_k$:\n",
    "   - run learning algorithm using $\\lambda = \\lambda_k$ and training set\n",
    "   - assess learnt model computing (R)MSE on validation set\n",
    "3. Run learning algorithm using $\\lambda = \\lambda^{\\text{opt}}$ (with $\\lambda^{\\text{opt}}$ corresponding to the lowest (R)MSE) and training$+$validation set\n",
    "4. Assess overall learning process computing (R)MSE on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational complexity for linear regression\n",
    "\n",
    "Remember that $w = \\left( X^T X \\right)^{-1} X^T y$\n",
    "\n",
    "* Time complexity: $\\mathrm O(n d^2 + d^3)$ basic operations\n",
    "  - $d^3$ for matrix inversion ($X$ is $n \\times d$, $X^T X$ is $d \\times d$)\n",
    "  - $n d^2$ for matrix multiplication\n",
    "* Space complexity: $\\mathrm O(n d + d^2)$ floats\n",
    "  - $d^2$ for $X^T X$ and its inverse\n",
    "  - $n d$ for $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big-scale regression\n",
    "\n",
    "Two scenarios:\n",
    "\n",
    "- big $n$, small $d$,\n",
    "- big $n$, big $d$\n",
    "- (what about small $n$ and big $d$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big $n$, small $d$\n",
    "\n",
    "* Time complexity $\\mathrm O(n d^2 + d^3)$\n",
    "  - $\\mathrm O(d^3)$ for matrix inversion is acceptable\n",
    "* Space complexity $\\mathrm O(nd + d^2)$\n",
    "  - $\\mathrm O(d^2)$ for $X^T X$ storage is acceptable\n",
    "* Instead, computation of $X^T X$ and storage of $X$ are bottlenecks\n",
    "\n",
    "Solution\n",
    "\n",
    "* Distribute storage of $X$ across cluster nodes\n",
    "* Express $X^T X$ as a sum of outer products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix product through outer products\n",
    "\n",
    "Let $A$ be a $n \\times d$ matrix and $B$ be a $d \\times m$ matrix:\n",
    "\n",
    "$$ (AB)_{ij} = \\sum_{k=1}^d a_{ik} b_{kj} $$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$ AB = \\sum_{k=1}^d P_k $$\n",
    "\n",
    "where $(P_k)_{ij} = a_{ik} b_{kj}$, which means that $P_k = A_k \\oplus B^k$ (outer product of $k$-th column of $A$ and $k$-th row of $B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An example\n",
    "\n",
    "\\begin{align}\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "9 & 3 & 5 \\\\\n",
    "4 & 1 & 2 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "3 & -5 \\\\\n",
    "2 & 3\n",
    "\\end{array}\n",
    "\\right]\n",
    "&=\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "9 \\\\ 4\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "1 \\\\ 2\n",
    "\\end{array}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "3 \\\\ 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "3 \\\\ -5\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "5 \\\\ 2\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\oplus\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "2 \\\\ 3\n",
    "\\end{array}\n",
    "\\right]\\\\\n",
    "&=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "9 & 18 \\\\\n",
    "4 & 8\n",
    "\\end{array}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "9 & -15 \\\\\n",
    "3 & -5\n",
    "\\end{array}\n",
    "\\right]\n",
    "+\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "10 & 15 \\\\\n",
    "4 & 6\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "28 & 18 \\\\\n",
    "11 & 9 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed computation of $X^T X$\n",
    "\n",
    "* Compute $X^T X$ as $\\sum_{k=1}^d X^T_k \\oplus X^k = \\sum_{k=1}^d x_k \\cdot x_k$\n",
    "  - requires local storage of $\\mathrm O(d^2)$, local computation of $\\mathrm O(d^2)$\n",
    "* Compute $(X^T X)^{-1}$ summing local results and inverting\n",
    "  - requires local storage of $\\mathrm O(d^2)$, local computation of $\\mathrm O(d^3)$\n",
    "\n",
    "``train.map(computer_outer_product)\n",
    "     .reduce(sum_and_invert)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big $d$, big $n$\n",
    "\n",
    "* Time complexity $\\mathrm O(n d^2 + d^3)$\n",
    "  - also matrix inversion is a bottleneck\n",
    "* Space complexity $\\mathrm O(nd + d^2)$\n",
    "  - also storage of $X^T X$ is a bottleneck\n",
    "* And of course previous bottlenecks are still there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big $d$, big $n$\n",
    "\n",
    "Solution\n",
    "\n",
    "* A different approach to linear regression\n",
    "  - Rule of thumb: computation and storage should be linear in $d$ and $n$\n",
    "* Exploit sparsity\n",
    "  - explicit\n",
    "  - implicit\n",
    "* Use a different algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "Assume $f: \\mathbb R \\mapsto \\mathbb R$\n",
    "\n",
    "1. Choose $x_0 \\in \\mathbb R$, set $i=0$\n",
    "2. Repeat until convergence\n",
    "   - $x_{i+1} = x_i - f'(x_i)$\n",
    "   - $i = i+1$\n",
    "   \n",
    "This algorithm converges to a *local* minimum $x^*$ for $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "If $f : \\mathbb R^d \\mapsto \\mathbb R$\n",
    "\n",
    "1. Choose $x_0 \\in \\mathbb R^d$, set $i=0$\n",
    "2. Repeat until convergence\n",
    "   - $x_{i+1} = x_i - \\nabla f(x_i)$\n",
    "   - $i = i+1$\n",
    "\n",
    "Where $\\nabla f(x) \\in \\mathbb R^d$ with $\\nabla f(x)_i = \\partial f(x) / \\partial x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "Critical issues\n",
    "\n",
    "* Choosing initial point\n",
    "* Setting step size (*learning rate*)\n",
    "\n",
    "$x_{i+1} = x_i - \\eta f'(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent for linear regression\n",
    "\n",
    "$$ w = \\arg \\min_w \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2 $$\n",
    "\n",
    "Thus $f(w) = \\sum_{j=1}^n \\left( w \\cdot x^{(j)} - y^{(j)} \\right)^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w_i}(w) = \\sum_{j=1}^n 2 \\left(w \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w}(w) = \\sum_{j=1}^n 2 \\left(w \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)}\n",
    "$$\n",
    "\n",
    "Local minimization through gradient descent\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta \\sum_{j=1}^n \\left(w_t \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convexity\n",
    "\n",
    "Linear regression is a convex problem, so gradient descent is OK\n",
    "\n",
    "# Dynamic learning rate\n",
    "\n",
    "$$\\eta \\mapsto \\eta_t = \\frac{\\eta}{n \\sqrt{t}}$$\n",
    "\n",
    "* Big steps at the beginning of iteration\n",
    "* Small steps as we reach convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallelization of gradient descent\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta_t \\sum_{j=1}^n \\left(w \\cdot x^{(j)} - y^{(j)} \\right) x^{(j)} $$\n",
    "\n",
    "* Send $w_t$ to all workers\n",
    "* Compute summands in parallel\n",
    "* Now each worker stores $w_t$ and $x^{(j)}$ (space complexity is $\\mathrm O(d)$)\n",
    "* Computation is $\\mathrm O(d)$, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification problem\n",
    "\n",
    "Not that different from the ML framing of regression\n",
    "\n",
    "* Data: a set $\\{ (x^{(1)}, y^{(1)}), \\dots, (x^{(n)}, y^{(n)}) \\}$ of associations between *objects* and *labels*\n",
    "* Goal: find a mapping from objects to labels\n",
    "  - describing observed data within a reasonable approximation level\n",
    "  - generalizing to unseen observations\n",
    "* Technically, a supervised learning problem\n",
    "\n",
    "Now, labels belong to a *discrete* set\n",
    "\n",
    "* positive/negative (binary classification, we'll stick on this)\n",
    "* multi-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear classification\n",
    "\n",
    "* Use something similar to regression in order to find two half-spaces for objects\n",
    "* Classify according to the half-space where objects belong\n",
    "\n",
    "$$ \\hat y = \\mathrm{sign}(w x) $$\n",
    "\n",
    "* Note that $y \\in \\{ -1, 1 \\}$ in order for $\\hat y$ be reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating predictions\n",
    "\n",
    "* In regression: $L_2$ loss $\\left( \\hat y - y \\right)^2$\n",
    "* In binary classification: $0/1$ loss\n",
    "  - null penalty in case of correct classification\n",
    "  - unitary penalty in case of misclassification\n",
    "* Let $z = \\hat y y$\n",
    "$$\n",
    "\\ell_{0/1}(z) = \\left\\{\n",
    "\\begin{array}{cc}\n",
    "1 & \\text{if } z < 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning the classifier\n",
    "\n",
    "* In regression $w = \\arg \\min_w \\ell(w)$\n",
    "* In our case\n",
    "\n",
    "\\begin{align}\n",
    "w &= \\arg \\min \\ell_{0/1}(w) \\\\\n",
    "  &= \\arg \\min \\sum_{j=1}^n \\ell_{0/1}\\left(y^{(j)} w x^{(j)}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A problem with convexity\n",
    "\n",
    "$0/1$ loss is not convex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def zero_one_loss(z):\n",
    "    return 0 if z >= 0 else 1\n",
    "\n",
    "n = 20\n",
    "z = np.arange(-5, 10, .1)\n",
    "g = plt.plot(z, map(zero_one_loss, z))\n",
    "plt.ylim([-1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approximate loss\n",
    "\n",
    "* Need for convexity\n",
    "* Between various possibilities, *log loss*\n",
    "\n",
    "$$ \\ell_{\\log}(z) = \\log \\left( 1 + \\mathrm e^{-z} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_loss(z):\n",
    "    return np.log(1 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-3, 10, .5)\n",
    "g = plt.plot(z, map(log_loss, z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Result: logistic regression\n",
    "\n",
    "$$ w = \\arg \\min_w \\sum_{j=1}^n \\ell_{\\log}\\left(y^{(j) w x^{(i)}} \\right) $$\n",
    "\n",
    "Thus we are optimizing\n",
    "\n",
    "$$ f(w) = \\sum_{j=1}^n \\ell_{\\log}\\left(y^{(j) w x^{(i)}} \\right) $$\n",
    "\n",
    "* Optimization through gradient descent\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta \\nabla f(w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computing derivatives\n",
    "\n",
    "As\n",
    "\n",
    "$$ \\frac{\\partial \\ell_{\\log(z)}}{\\partial z} =\n",
    "   \\frac{-\\mathrm e^{-z}}{1 + \\mathrm e^{-z}} =\n",
    "   -\\left( 1 - \\frac{1}{1 + \\mathrm e^{-z}} \\right) $$\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial w_k} &=\n",
    "\\sum_{j=1}^n \\left[ 1- \\frac{1}{1+ \\mathrm e^{ -y^{(j)} w x^{(j)} } } \\right] \\left( -y^{(j)} x^{(j)_k} \\right)\n",
    "\\end{align}\n",
    "\n",
    "Thus\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial w} &=\n",
    "\\sum_{j=1}^n \\left( 1- \\frac{1}{1+ \\mathrm e^{ -y^{(j)} w x^{(j)} } } \\right) \\left( -y^{(j)} x^{(j)} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularized logistic regression\n",
    "\n",
    "As in ridge regression, add a regularization term\n",
    "\n",
    "$$ \\min_w \\sum_{j=1}^n \\ell_{\\log}\\left(y^{(j) w x^{(i)}} \\right) + \\lambda || w ||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic interpretation\n",
    "\n",
    "* A step ahead, instead of predicting class for an object $x$...\n",
    "* ...estimate the probability of a class *given* the object\n",
    "\n",
    "$$\\mathrm P(Y=1 | X=x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic interpretation\n",
    "\n",
    "* Can't use linear regression: $\\mathrm P(Y=1 | X=x) \\neq wx$\n",
    "  - because probabilities belong to $[0, 1]$\n",
    "* Can't use sign: $\\mathrm P(Y=1 | X=x) \\neq \\mathrm{sign}(wx)$\n",
    "  - for same resaon of before\n",
    "* Can use logistic function: $\\mathrm P(Y=1 | X=x) = \\sigma(wx)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic function\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\mathrm e^{-z}} $$\n",
    "\n",
    "kinda of smooth approximation of a step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1. / (1 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-7, 7, .5)\n",
    "g = plt.plot(z, map(logistic, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predicting probabilities\n",
    "\n",
    "1. Use logistic regression to learn $w$\n",
    "2. Predict probabilities as $\\mathrm P(Y=1 | X=x) = \\sigma(wx)$\n",
    "\n",
    "# Classifying using probabilities\n",
    "\n",
    "Threshold probability: $x$ is positive if $\\mathrm P(Y=1 | X=x) > 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing threshold: ROC curves\n",
    "\n",
    "Two kind of errors:\n",
    "\n",
    "* false positives (FP): objects classified as positive when they are negative\n",
    "* false negatives (FN): objects classified as negative when they are positive\n",
    "\n",
    "$$ \\mathrm{TPR} = \\frac{TP}{TP+FN}, \\quad \\mathrm{FPR} = \\frac{FP}{FP+TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing threshold: ROC curves\n",
    "\n",
    "* threshold = 0: everything is positive, FN=TN=0\n",
    "* threshold = 1: everything is negative, FP=TP=0\n",
    "\n",
    "![ROC curve](img/roc.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
